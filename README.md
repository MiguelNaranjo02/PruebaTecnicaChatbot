# ü§ñ Chatbot Inteligente - Browser Travel Solution

## Descripci√≥n del Proyecto
Este proyecto implementa un **chatbot inteligente** para una empresa de servicios tecnol√≥gicos.  
El objetivo es mejorar la atenci√≥n al cliente 24/7, reducir la carga del equipo de soporte y ofrecer respuestas r√°pidas y contextuales a preguntas frecuentes.

El sistema:
- Responde consultas frecuentes sobre productos y servicios.
- Clasifica consultas en categor√≠as como **ventas, soporte e informaci√≥n general**.
- Deriva conversaciones complejas a agentes humanos.
- Mantiene el **historial de conversaci√≥n** y el **contexto**.
- Puede integrarse en una web o plataforma de terceros a trav√©s de una API REST.

---

#  Instrucciones de Instalaci√≥n

### 1. Clonar el repositorio

git clone https://github.com/MiguelNaranjo02/PruebaTecnicaChatbot
cd chatbot_project

### 2. Crear y activar entorno virtual (recomendado con conda o venv)
conda create -n chatbot_env python=3.10 -y
conda activate chatbot_env

### 3. pip install -r requirements.txt
Dependencias principales:

- fastapi
- uvicorn
- pandas
- langchain-community
- langchain-huggingface.
- faiss-cpu
### 4. Crear estructura de datos
Dentro del directorio data/ coloca:
- faq.csv con preguntas frecuentes.
- knowledge.md con documentaci√≥n extendida.
  
#GUIA DE USO
###1. Levantar el backend:
- En la terminal ejecutar:
- uvicorn src.api.server:app --reload --host 127.0.0.1 --port 8000

###2. Ejecutar LM Studio
- (descargar un modelo ej:Meta-Llama-3-8B-Instruct y correrlo en http://127.0.0.1:1234)

###3. Abrir la interfaz de chat: En la terminal ejecutar:
- streamlit run "path al archivo app.py"
- Esto abrir√° una interfaz web simple para interactuar con el chatbot.

###4. Ejemplo de uso
- Usuario: "¬øQu√© servicios ofrecen?"
- A lo que el Chatbot respondera: "Ofrecemos desarrollo de software, consultor√≠a en datos e implementaci√≥n de IA."

##Explicacion de Arquitectura:
El proyecto est√° organizado en m√≥dulos:

- config.py ‚Üí carga la configuraci√≥n desde config.yaml.
- logger.py ‚Üí logging centralizado para depuraci√≥n y monitoreo.
- knowledge_base.py ‚Üí gestiona la base de conocimientos, carga CSV/Markdown, indexa datos con FAISS + HuggingFaceEmbeddings.
- retriever.py ‚Üí recupera informaci√≥n relevante de la base de conocimientos.
- chatbot.py ‚Üí l√≥gica principal del chatbot, integra el LLM y el contexto.
- server.py ‚Üí expone API REST con FastAPI para integraciones externas.
- app.py ‚Üí interfaz de usuario (chat web simple).

###Flujo:

- Usuario env√≠a consulta ‚Üí app.py.
- API recibe mensaje ‚Üí server.py.
- Chatbot procesa ‚Üí chatbot.py.
- Recupera contexto ‚Üí retriever.py + knowledge_base.py.
- Respuesta generada por LLM.
- Respuesta devuelta al usuario.

##Tecnolog√≠as Utilizadas

- Python 3.10 ‚Üí lenguaje principal.
- FastAPI ‚Üí framework backend, expone API REST.
- Uvicorn ‚Üí servidor ASGI r√°pido para producci√≥n.
- LangChain ‚Üí manejo de embeddings y retrieval.
- HuggingFace Embeddings ‚Üí modelo all-MiniLM-L6-v2 para vectorizaci√≥n de textos.
- FAISS ‚Üí motor de b√∫squeda vectorial para indexaci√≥n r√°pida.
- Logging ‚Üí trazabilidad y depuraci√≥n.

##Justificaci√≥n:

- FastAPI y Uvicorn por su rapidez y compatibilidad con microservicios.
- LangChain y HuggingFace para procesamiento de lenguaje natural.
- FAISS por ser eficiente en b√∫squedas sem√°nticas.

##Dificultades Encontradas y Soluciones

- Problemas con dependencias de embeddings (sentence-transformers): 
Soluci√≥n: instalar con pip install sentence-transformers.

- Error al inicializar FAISS:
Soluci√≥n: instalar versi√≥n adecuada seg√∫n el entorno (faiss-cpu o faiss-gpu).

- Confusi√≥n entre Chroma y FAISS en LangChain:
Soluci√≥n: mantener FAISS como vectorstore principal, ajustando knowledge_base.py.

- Error retriever.retrieve:
El VectorStoreRetriever no tiene .retrieve, solo .get_relevant_documents.

- Manejo de rutas relativas (config.yaml, data/faq.csv):
Ajustar rutas absolutas o asegurar ejecuci√≥n desde ra√≠z del proyecto.


